{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45d33ff3-1760-4701-897a-4904c1050b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import contractions\n",
    "import multiprocessing as mp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from spacy.tokens import Doc\n",
    "import cupy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "# Initialize spaCy with efficient settings\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"lemmatizer\"], exclude=[\"cupy\"])\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Configuration\n",
    "NUM_ASPECTS = 26\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TOKENIZER = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "# Aspect Seed Keywords\n",
    "ASPECT_SEEDS = {\n",
    "    \"usability\": [\"interface\", \"ui\", \"ease\", \"easy\", \"use\", \"navigate\", \"use\", \"friendly\", \"intuitive\", \"clumsy\"],\n",
    "    \"performance\": [\"speed\", \"fast\", \"slow\", \"crash\", \"freeze\", \"lag\", \"performance\"],\n",
    "    \"pricing\": [\"price\", \"cost\", \"expensive\", \"cheap\", \"deal\", \"value\", \"overpriced\"],\n",
    "    \"installation\": [\"install\", \"setup\", \"download\", \"activate\", \"registration\"],\n",
    "    \"features\": [\"feature\", \"functionality\", \"option\", \"tool\", \"missing\", \"limited\"],\n",
    "    \"support\": [\"customer\", \"service\", \"support\", \"help\", \"response\"],\n",
    "    \"compatibility\": [\"compatible\", \"work\", \"windows\", \"mac\", \"version\"],\n",
    "    \"security\": [\"virus\", \"malware\", \"hack\", \"secure\", \"protection\"],\n",
    "    \"design\": [\"design\", \"aesthetic\", \"look\", \"feel\", \"appearance\", \"modern\", \"outdated\", \"theme\", \"color\"],\n",
    "    \"update\": [\"update\", \"upgrade\", \"patch\", \"version\", \"release\", \"changelog\", \"fixes\", \"improvement\", \"bugfix\"],\n",
    "    \"reliability\": [\"reliable\", \"unreliable\", \"consistent\", \"crash\", \"bug\", \"stable\", \"error\", \"issue\"],\n",
    "    \"accessibility\": [\"accessible\", \"accessibility\", \"screen reader\", \"font size\", \"color blind\", \"contrast\", \"voice control\"],\n",
    "    \"integration\": [\"integrate\", \"integration\", \"connect\", \"API\", \"sync\", \"third-party\", \"external\", \"plugin\"],\n",
    "    \"learning_curve\": [\"learn\", \"tutorial\", \"guide\", \"documentation\", \"manual\", \"training\", \"hard to learn\", \"easy to learn\"],\n",
    "    \"account\": [\"account\", \"login\", \"signup\", \"password\", \"authentication\", \"profile\", \"credentials\", \"logout\"],\n",
    "    \"documentation\": [\"documentation\", \"manual\", \"instructions\", \"readme\", \"guide\", \"faq\", \"reference\", \"how-to\"],\n",
    "    \"availability\": [\"available\", \"availability\", \"download\", \"region\", \"country\", \"store\", \"platform\", \"restricted\"],\n",
    "    \"licensing\": [\"license\", \"licensed\", \"open-source\", \"freeware\", \"trial\", \"terms\", \"agreement\", \"policy\"],\n",
    "    \"interface_language\": [\"language\", \"translate\", \"localization\", \"multilingual\", \"support language\", \"region\"],\n",
    "    \"update_policy\": [\"update frequency\", \"auto-update\", \"manual update\", \"changelog\", \"version history\"],\n",
    "    \"usage_context\": [\"business\", \"enterprise\", \"education\", \"home\", \"personal\", \"professional\", \"students\"],\n",
    "    \"device_requirements\": [\"ram\", \"storage\", \"requirement\", \"specs\", \"CPU\", \"memory\", \"disk space\"],\n",
    "    \"time_usage\": [\"hours\", \"duration\", \"short term\", \"long term\", \"daily\", \"weekly\", \"immediate\"],\n",
    "    \"audience\": [\"beginner\", \"novice\", \"intermediate\", \"expert\", \"developer\", \"admin\", \"non-technical\"],\n",
    "    \"availability_status\": [\"beta\", \"alpha\", \"release\", \"preview\", \"stable\", \"deprecated\"],\n",
    "    \"branding\": [\"brand\", \"logo\", \"company\", \"publisher\", \"vendor\", \"developer\"]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5454c518-1d59-419d-ae3c-bc3d75d81d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = contractions.fix(text.lower())\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9668c142-611c-4076-8dff-252147b43690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Aspect Extraction with Parallel Processing\n",
    "def extract_aspect_terms_cpu(texts, batch_size=32):\n",
    "    aspect_results = []\n",
    "    \n",
    "    # Process in batches with progress bar\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=batch_size), \n",
    "                   total=len(texts),\n",
    "                   desc=\"Extracting aspects\"):\n",
    "        terms = []\n",
    "        \n",
    "        # Extract noun phrases\n",
    "        for chunk in doc.noun_chunks:\n",
    "            term = chunk.text.lower()\n",
    "            for aspect, keywords in ASPECT_SEEDS.items():\n",
    "                if any(keyword in term for keyword in keywords):\n",
    "                    terms.append((term, aspect))\n",
    "        \n",
    "        # Extract verbs\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                for aspect, keywords in ASPECT_SEEDS.items():\n",
    "                    if any(keyword in token.text.lower() for keyword in keywords):\n",
    "                        terms.append((token.text.lower(), aspect))\n",
    "        \n",
    "        aspect_results.append(terms if terms else [(\"general\", \"general\")])\n",
    "    \n",
    "    return aspect_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a35a32-c96d-4dcd-91c3-76fc79e98969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading with Chunking Support\n",
    "def load_and_preprocess(file_path, chunksize=None):\n",
    "    \"\"\"Load and preprocess dataset with optional chunking for large files\"\"\"\n",
    "    if chunksize:\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "            chunk = chunk.dropna(subset=['reviewText']).drop_duplicates(subset=['reviewText'])\n",
    "            chunk['cleaned_text'] = chunk['reviewText'].apply(preprocess_text)\n",
    "            chunks.append(chunk)\n",
    "        return pd.concat(chunks)\n",
    "    else:\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = data.dropna(subset=['reviewText']).drop_duplicates(subset=['reviewText'])\n",
    "        data['cleaned_text'] = data['reviewText'].apply(preprocess_text)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3cb6985f-b51f-4d83-abf4-11376fa3ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering Aspects\n",
    "def cluster_aspects(data, n_clusters=NUM_ASPECTS, return_models=False):\n",
    "    \n",
    "    texts = data['cleaned_text'].tolist()\n",
    "    \n",
    "    # Use CPU extraction\n",
    "    aspect_results = extract_aspect_terms_cpu(texts)\n",
    "    data['extracted_aspects'] = aspect_results\n",
    "    \n",
    "    all_terms = [term for sublist in aspect_results for term, _ in sublist if term != 'general']\n",
    "    \n",
    "    if not all_terms:\n",
    "        data['final_aspects'] = [['general']] * len(data)\n",
    "        return data\n",
    "    \n",
    "    # Sparse matrix operations\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_terms)\n",
    "    \n",
    "    # MiniBatchKMeans handles sparse matrices well\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=min(n_clusters, len(all_terms)),\n",
    "        random_state=42,\n",
    "        batch_size=1000\n",
    "    )\n",
    "    clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    # Aspect naming\n",
    "    term_to_cluster = defaultdict(list)\n",
    "    for term, cluster_id in zip(all_terms, clusters):\n",
    "        term_to_cluster[cluster_id].append(term)\n",
    "    \n",
    "    aspect_names = {}\n",
    "    for cluster_id, terms in term_to_cluster.items():\n",
    "        scores = []\n",
    "        for aspect, keywords in ASPECT_SEEDS.items():\n",
    "            exact_matches = sum(1 for term in terms if any(keyword == term for keyword in keywords))\n",
    "            partial_matches = sum(1 for term in terms if any(keyword in term for keyword in keywords))\n",
    "            score = 0.7 * exact_matches + 0.3 * partial_matches\n",
    "            scores.append((aspect, score))\n",
    "        \n",
    "        best_aspect = max(scores, key=lambda x: x[1])[0] if max(scores, key=lambda x: x[1])[1] > 0 else \"general\"\n",
    "        aspect_names[cluster_id] = best_aspect\n",
    "    \n",
    "    def map_to_aspect(terms):\n",
    "        if not terms or terms[0][0] == 'general':\n",
    "            return ['general']\n",
    "        term_vecs = vectorizer.transform([term for term, _ in terms if term != 'general'])\n",
    "        return list(set(aspect_names[pred] for pred in kmeans.predict(term_vecs)))\n",
    "    \n",
    "    data['final_aspects'] = data['extracted_aspects'].apply(map_to_aspect)\n",
    "    return (data, vectorizer, kmeans) if return_models else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cee8c7d6-3098-4c95-90e4-1a5d3a588cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Aspect Discovery\n",
    "def discover_new_aspects(texts, top_n=20):\n",
    "\n",
    "    noun_phrases = []\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        noun_phrases.extend([chunk.text.lower() for chunk in doc.noun_chunks])\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features=1000, ngram_range=(1, 3))\n",
    "    X = vectorizer.fit_transform(noun_phrases)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    counts = np.array(X.sum(axis=0))[0]\n",
    "    \n",
    "    existing_terms = set().union(*ASPECT_SEEDS.values())\n",
    "    new_terms = [(term, count) for term, count in zip(terms, counts) \n",
    "                if term not in existing_terms and count > 10]\n",
    "    \n",
    "    return sorted(new_terms, key=lambda x: x[1], reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04efd0f-dcd8-4bcf-8766-8f3a8899194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ABSA Dataset\n",
    "def prepare_absa_dataset(data):\n",
    "    aspect_data = []\n",
    "    for idx, row in data.iterrows():\n",
    "        for aspect in row['final_aspects']:\n",
    "            aspect_data.append({\n",
    "                'text': row['reviewText'],\n",
    "                'aspect': aspect,\n",
    "                'label': int(row['overall'] + 1)  # Map -1->0, 0->1, 1->2\n",
    "            })\n",
    "    aspect_df = pd.DataFrame(aspect_data)\n",
    "    train_df, val_df = train_test_split(aspect_df, test_size=0.2, random_state=42)\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c9d27c-c88c-447b-82be-aef8d28b0d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, num_labels):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embed_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_labels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [torch.max_pool1d(pool, pool.size(2)).squeeze(2) for pool in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(h_n[-1])\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_labels, dropout=0.3):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.bilstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=0.3)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        _, (h_n, _) = self.bilstm(x)\n",
    "        h_n = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        x = self.bn(h_n)\n",
    "        return self.fc(x)\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_labels, dropout=0.3):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1).cpu()\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            packed_out, _ = self.gru(packed)\n",
    "            gru_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        else:\n",
    "            gru_out, _ = self.gru(embedded)\n",
    "        pooled = torch.mean(gru_out, dim=1)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0db6d2e-385b-4f54-b8db-7784258f64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABSA Dataset and Training \n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=64):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.aspects = df['aspect'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        aspect = str(self.aspects[idx])\n",
    "        input_text = f\"{aspect} {text}\"\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "054364f8-c44b-48dd-98d7-187473714f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd93100-581b-4f4d-8c2e-e6d22b2e0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(models, dataloader, is_ensemble=False):\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            if is_ensemble:\n",
    "                preds = ensemble_predict(models, input_ids, attention_mask)\n",
    "            else:\n",
    "                model = models[0]\n",
    "                model.eval()\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return accuracy_score(true_labels, predictions), \\\n",
    "           precision_score(true_labels, predictions, average='weighted', zero_division=0), \\\n",
    "           recall_score(true_labels, predictions, average='weighted', zero_division=0), \\\n",
    "           f1_score(true_labels, predictions, average='weighted', zero_division=0), \\\n",
    "           confusion_matrix(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe28f23d-a40f-49f7-a8c4-59f42d478dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect Clustering for Single Text\n",
    "def cluster_aspects_for_text(text, vectorizer=None, kmeans=None):\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    aspect_results = extract_aspect_terms_cpu([cleaned_text])\n",
    "    data = pd.DataFrame({'cleaned_text': [cleaned_text], 'extracted_aspects': [aspect_results[0]]})\n",
    "    \n",
    "    # Extract terms safely\n",
    "    all_terms = []\n",
    "    for sublist in data['extracted_aspects']:\n",
    "        for item in sublist:\n",
    "            if isinstance(item, tuple) and len(item) == 2 and item[0] != 'unknown':\n",
    "                all_terms.append(item[0])\n",
    "            elif item == 'general':\n",
    "                all_terms.append('general')\n",
    "    \n",
    "    if not all_terms or all(t == 'general' for t in all_terms):\n",
    "        data['final_aspects'] = [['general']]\n",
    "        return data['final_aspects'].iloc[0]\n",
    "    \n",
    "    # Initialize or use provided vectorizer\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_terms)\n",
    "    else:\n",
    "        tfidf_matrix = vectorizer.transform(all_terms)\n",
    "    \n",
    "    # Use the number of aspects in ASPECT_SEEDS as maximum clusters\n",
    "    max_aspects = len(ASPECT_SEEDS)\n",
    "    \n",
    "    # Initialize or use provided KMeans\n",
    "    if kmeans is None:\n",
    "        n_clusters = min(max_aspects, len(set(all_terms)))  # Don't exceed number of defined aspects\n",
    "        kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "    else:\n",
    "        clusters = kmeans.predict(tfidf_matrix)\n",
    "        n_clusters = min(kmeans.n_clusters, max_aspects)  # Cap at our max aspects\n",
    "    \n",
    "    # Map terms to clusters\n",
    "    term_to_cluster = defaultdict(list)\n",
    "    for term, cluster_id in zip(all_terms, clusters):\n",
    "        term_to_cluster[cluster_id].append(term)\n",
    "    \n",
    "    # Name clusters based on seed terms - ensure we cover all possible cluster IDs\n",
    "    aspect_names = {}\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_terms = term_to_cluster.get(cluster_id, [])\n",
    "        max_overlap = 0\n",
    "        aspect_name = f\"aspect_{cluster_id}\"\n",
    "        \n",
    "        # Check against all aspect seeds\n",
    "        for aspect, seeds in ASPECT_SEEDS.items():\n",
    "            overlap = len(set(cluster_terms).intersection(seeds))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                aspect_name = aspect\n",
    "        \n",
    "        # If no strong match, try partial matches\n",
    "        if max_overlap == 0:\n",
    "            for aspect, seeds in ASPECT_SEEDS.items():\n",
    "                if any(seed in term for term in cluster_terms for seed in seeds):\n",
    "                    aspect_name = aspect\n",
    "                    break\n",
    "        \n",
    "        aspect_names[cluster_id] = aspect_name\n",
    "    \n",
    "    def map_to_cluster_aspect(terms):\n",
    "        if not terms:\n",
    "            return ['general']\n",
    "        final_aspects = []\n",
    "        for item in terms:\n",
    "            if isinstance(item, tuple) and len(item) == 2 and item[0] != 'unknown':\n",
    "                term = item[0]\n",
    "                try:\n",
    "                    prediction = kmeans.predict(vectorizer.transform([term]))[0]\n",
    "                    # Ensure prediction is within our aspect_names\n",
    "                    if prediction < n_clusters:  # Check cluster ID is valid\n",
    "                        final_aspects.append(aspect_names[prediction])\n",
    "                    else:\n",
    "                        # Find the most similar aspect\n",
    "                        term_vec = vectorizer.transform([term])\n",
    "                        closest_cluster = kmeans.predict(term_vec)[0]\n",
    "                        if closest_cluster < n_clusters:\n",
    "                            final_aspects.append(aspect_names[closest_cluster])\n",
    "                        else:\n",
    "                            final_aspects.append('general')\n",
    "                except:\n",
    "                    final_aspects.append('general')\n",
    "            elif item == 'general':\n",
    "                final_aspects.append('general')\n",
    "        return list(set(final_aspects)) if final_aspects else ['general']\n",
    "    \n",
    "    data['final_aspects'] = data['extracted_aspects'].apply(map_to_cluster_aspect)\n",
    "    return data['final_aspects'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fadf959-6a8c-4fdd-a152-90763cfc812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, input_ids, attention_mask=None, T=2.0):\n",
    "    num_classes = models[0].fc.out_features\n",
    "    votes = np.zeros((input_ids.shape[0], num_classes))\n",
    "    with torch.no_grad():\n",
    "        layer_norm = torch.nn.LayerNorm(num_classes).to(DEVICE)\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = layer_norm(logits)\n",
    "            logits /= T\n",
    "            votes += logits.cpu().numpy()\n",
    "    final_probs = torch.softmax(torch.tensor(votes), dim=1).numpy()\n",
    "    return np.argmax(final_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "100281b4-c69d-4628-843c-5ed8a048fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text_absa(text, aspects, tokenizer, models, max_length=64):\n",
    "    results = {}\n",
    "    for aspect in aspects:\n",
    "        input_text = f\"{aspect} {text}\"\n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(DEVICE)\n",
    "        attention_mask = encoding['attention_mask'].to(DEVICE)\n",
    "        pred = ensemble_predict(models, input_ids, attention_mask)[0]\n",
    "        results[aspect] = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}[pred]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66eb8eb8-4799-4ff3-a1b0-b6bd1762bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess(\"../dataset/Amazon/software/combined_output.csv\", chunksize=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c360a8a-7641-49a0-8c52-5be8871147a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered new terms: [('software', 33748), ('product', 29262), ('program', 26185), ('computer', 17389), ('one', 15058), ('new', 14214), ('great', 13777), ('time', 13258), ('good', 12030), ('years', 9192), ('tax', 9001), ('money', 8925), ('amazon', 8239), ('problem', 7850), ('many', 7719), ('system', 7395), ('norton', 7243), ('pc', 7238), ('quicken', 7237), ('microsoft', 7156)]\n"
     ]
    }
   ],
   "source": [
    "    # Discover new aspects\n",
    "    new_terms = discover_new_aspects(data['cleaned_text'].sample(100000).tolist())\n",
    "    print(\"Discovered new terms:\", new_terms)\n",
    "    \n",
    "     # Incorporate new terms into ASPECT_SEEDS\n",
    "    for term, count in new_terms:\n",
    "        if count > 50:  # Only add frequently occurring terms\n",
    "            if term not in set().union(*ASPECT_SEEDS.values()):\n",
    "                ASPECT_SEEDS.setdefault(\"new_\" + term, []).append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c7f68af-1da2-4c19-a592-1e83c5871a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting aspects: 100%|█████████████████████████████████████████████████████| 360261/360261 [23:59<00:00, 250.28it/s]\n"
     ]
    }
   ],
   "source": [
    "    # Cluster aspects\n",
    "    data, vectorizer, kmeans = cluster_aspects(data, return_models=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c1c64ba-b7a3-4bf2-806b-40e9f25f728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Save TF-IDF vectorizer\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    \n",
    "    # Save KMeans clustering model\n",
    "    with open('kmeans_model.pkl', 'wb') as f:\n",
    "        pickle.dump(kmeans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e0b27cd-0a6b-45d6-b949-253f49cef76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Prepare dataset for training\n",
    "    train_df, val_df = prepare_absa_dataset(data)\n",
    "    train_dataset = ABSADataset(train_df, TOKENIZER, MAX_LENGTH)\n",
    "    val_dataset = ABSADataset(val_df, TOKENIZER, MAX_LENGTH)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ded4a5c-d846-43e2-b5ee-4a606f85911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize models\n",
    "    VOCAB_SIZE = TOKENIZER.vocab_size\n",
    "    EMBED_DIM = 100\n",
    "    NUM_FILTERS = 100\n",
    "    FILTER_SIZES = [2, 3, 4]\n",
    "    HIDDEN_DIM = 64\n",
    "    NUM_LAYERS = 2\n",
    "    NUM_LABELS = 3\n",
    "    EPOCHS = 30\n",
    "\n",
    "    models = [\n",
    "        CNNModel(VOCAB_SIZE, EMBED_DIM, NUM_FILTERS, FILTER_SIZES, NUM_LABELS).to(DEVICE),\n",
    "        LSTMModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LABELS).to(DEVICE),\n",
    "        BiLSTMModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, NUM_LABELS).to(DEVICE),\n",
    "        GRUModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, NUM_LABELS).to(DEVICE)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24d11fab-ab20-47bb-a1ea-74d30c021f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.array([0, 1, 2]),\n",
    "        y=train_df['label'].values\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcf43150-7263-4285-9440-726bbc95f5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNNModel...\n",
      "Epoch 1/30, Train Loss: 1.0120\n",
      "Validation - Accuracy: 0.6447, Precision: 0.6960, Recall: 0.6447, F1: 0.6647\n",
      "Confusion Matrix:\n",
      "[[43805 10141 10924]\n",
      " [ 5661  5586  4780]\n",
      " [17394 14522 65711]]\n",
      "Epoch 2/30, Train Loss: 0.9378\n",
      "Validation - Accuracy: 0.6628, Precision: 0.7276, Recall: 0.6628, F1: 0.6861\n",
      "Confusion Matrix:\n",
      "[[46594 10424  7852]\n",
      " [ 5736  6381  3910]\n",
      " [16313 15961 65353]]\n",
      "Epoch 3/30, Train Loss: 0.8985\n",
      "Validation - Accuracy: 0.6817, Precision: 0.7433, Recall: 0.6817, F1: 0.7045\n",
      "Confusion Matrix:\n",
      "[[47170 10413  7287]\n",
      " [ 5437  6723  3867]\n",
      " [14581 15247 67799]]\n",
      "Epoch 4/30, Train Loss: 0.8698\n",
      "Validation - Accuracy: 0.6998, Precision: 0.7555, Recall: 0.6998, F1: 0.7213\n",
      "Confusion Matrix:\n",
      "[[47196 10360  7314]\n",
      " [ 5067  6967  3993]\n",
      " [12654 14205 70768]]\n",
      "Epoch 5/30, Train Loss: 0.8469\n",
      "Validation - Accuracy: 0.6955, Precision: 0.7708, Recall: 0.6955, F1: 0.7226\n",
      "Confusion Matrix:\n",
      "[[47710 11426  5734]\n",
      " [ 4839  7963  3225]\n",
      " [12259 16880 68488]]\n",
      "Epoch 6/30, Train Loss: 0.8263\n",
      "Validation - Accuracy: 0.7150, Precision: 0.7758, Recall: 0.7150, F1: 0.7382\n",
      "Confusion Matrix:\n",
      "[[47298 10974  6598]\n",
      " [ 4519  7864  3644]\n",
      " [10556 14595 72476]]\n",
      "Epoch 7/30, Train Loss: 0.8085\n",
      "Validation - Accuracy: 0.7192, Precision: 0.7836, Recall: 0.7192, F1: 0.7437\n",
      "Confusion Matrix:\n",
      "[[47036 11520  6314]\n",
      " [ 4266  8241  3520]\n",
      " [ 9703 14805 73119]]\n",
      "Epoch 8/30, Train Loss: 0.7921\n",
      "Validation - Accuracy: 0.7154, Precision: 0.7922, Recall: 0.7154, F1: 0.7432\n",
      "Confusion Matrix:\n",
      "[[47911 11813  5146]\n",
      " [ 4269  8815  2943]\n",
      " [ 9974 16667 70986]]\n",
      "Epoch 9/30, Train Loss: 0.7763\n",
      "Validation - Accuracy: 0.7245, Precision: 0.7978, Recall: 0.7245, F1: 0.7515\n",
      "Confusion Matrix:\n",
      "[[45877 12952  6041]\n",
      " [ 3531  9221  3275]\n",
      " [ 8101 15292 74234]]\n",
      "Epoch 10/30, Train Loss: 0.7620\n",
      "Validation - Accuracy: 0.7406, Precision: 0.7955, Recall: 0.7406, F1: 0.7613\n",
      "Confusion Matrix:\n",
      "[[49349  9943  5578]\n",
      " [ 4415  8433  3179]\n",
      " [ 9730 13460 74437]]\n",
      "Epoch 11/30, Train Loss: 0.7483\n",
      "Validation - Accuracy: 0.7357, Precision: 0.8035, Recall: 0.7357, F1: 0.7601\n",
      "Confusion Matrix:\n",
      "[[49181 10872  4817]\n",
      " [ 4107  9181  2739]\n",
      " [ 9533 15107 72987]]\n",
      "Epoch 12/30, Train Loss: 0.7351\n",
      "Validation - Accuracy: 0.7415, Precision: 0.8072, Recall: 0.7415, F1: 0.7654\n",
      "Confusion Matrix:\n",
      "[[48659 11160  5051]\n",
      " [ 3802  9389  2836]\n",
      " [ 8765 14539 74323]]\n",
      "Epoch 13/30, Train Loss: 0.7217\n",
      "Validation - Accuracy: 0.7487, Precision: 0.8096, Recall: 0.7487, F1: 0.7710\n",
      "Confusion Matrix:\n",
      "[[49011 10734  5125]\n",
      " [ 3740  9428  2859]\n",
      " [ 8607 13792 75228]]\n",
      "Epoch 14/30, Train Loss: 0.7092\n",
      "Validation - Accuracy: 0.7526, Precision: 0.8133, Recall: 0.7526, F1: 0.7748\n",
      "Confusion Matrix:\n",
      "[[48361 11116  5393]\n",
      " [ 3383  9735  2909]\n",
      " [ 7894 13471 76262]]\n",
      "Epoch 15/30, Train Loss: 0.6978\n",
      "Validation - Accuracy: 0.7552, Precision: 0.8157, Recall: 0.7552, F1: 0.7772\n",
      "Confusion Matrix:\n",
      "[[49195 10640  5035]\n",
      " [ 3513  9814  2700]\n",
      " [ 8166 13645 75816]]\n",
      "Epoch 16/30, Train Loss: 0.6861\n",
      "Validation - Accuracy: 0.7599, Precision: 0.8179, Recall: 0.7599, F1: 0.7808\n",
      "Confusion Matrix:\n",
      "[[49914 10128  4828]\n",
      " [ 3549  9885  2593]\n",
      " [ 8393 13369 75865]]\n",
      "Epoch 17/30, Train Loss: 0.6762\n",
      "Validation - Accuracy: 0.7656, Precision: 0.8209, Recall: 0.7656, F1: 0.7856\n",
      "Confusion Matrix:\n",
      "[[49359 10306  5205]\n",
      " [ 3243 10062  2722]\n",
      " [ 7670 12707 77250]]\n",
      "Epoch 18/30, Train Loss: 0.6657\n",
      "Validation - Accuracy: 0.7683, Precision: 0.8240, Recall: 0.7683, F1: 0.7882\n",
      "Confusion Matrix:\n",
      "[[49543 10195  5132]\n",
      " [ 3141 10324  2562]\n",
      " [ 7563 12778 77286]]\n",
      "Epoch 19/30, Train Loss: 0.6537\n",
      "Validation - Accuracy: 0.7773, Precision: 0.8238, Recall: 0.7773, F1: 0.7942\n",
      "Confusion Matrix:\n",
      "[[50801  9019  5050]\n",
      " [ 3402 10022  2603]\n",
      " [ 8066 11618 77943]]\n",
      "Epoch 20/30, Train Loss: 0.6445\n",
      "Validation - Accuracy: 0.7823, Precision: 0.8263, Recall: 0.7823, F1: 0.7983\n",
      "Confusion Matrix:\n",
      "[[50872  8832  5166]\n",
      " [ 3256 10144  2627]\n",
      " [ 7791 11192 78644]]\n",
      "Epoch 21/30, Train Loss: 0.6345\n",
      "Validation - Accuracy: 0.7798, Precision: 0.8313, Recall: 0.7798, F1: 0.7981\n",
      "Confusion Matrix:\n",
      "[[50551  9528  4791]\n",
      " [ 2936 10687  2404]\n",
      " [ 7541 12104 77982]]\n",
      "Epoch 22/30, Train Loss: 0.6255\n",
      "Validation - Accuracy: 0.7828, Precision: 0.8332, Recall: 0.7828, F1: 0.8006\n",
      "Confusion Matrix:\n",
      "[[50896  9308  4666]\n",
      " [ 2920 10778  2329]\n",
      " [ 7597 11951 78079]]\n",
      "Epoch 23/30, Train Loss: 0.6163\n",
      "Validation - Accuracy: 0.7914, Precision: 0.8335, Recall: 0.7914, F1: 0.8065\n",
      "Confusion Matrix:\n",
      "[[51634  8392  4844]\n",
      " [ 3013 10553  2461]\n",
      " [ 7680 10842 79105]]\n",
      "Epoch 24/30, Train Loss: 0.6078\n",
      "Validation - Accuracy: 0.7943, Precision: 0.8360, Recall: 0.7943, F1: 0.8092\n",
      "Confusion Matrix:\n",
      "[[51221  8591  5058]\n",
      " [ 2752 10802  2473]\n",
      " [ 7253 10595 79779]]\n",
      "Epoch 25/30, Train Loss: 0.5988\n",
      "Validation - Accuracy: 0.8019, Precision: 0.8361, Recall: 0.8019, F1: 0.8141\n",
      "Confusion Matrix:\n",
      "[[53011  7155  4704]\n",
      " [ 3149 10492  2386]\n",
      " [ 8122  9853 79652]]\n",
      "Epoch 26/30, Train Loss: 0.5907\n",
      "Validation - Accuracy: 0.8000, Precision: 0.8409, Recall: 0.8000, F1: 0.8144\n",
      "Confusion Matrix:\n",
      "[[51393  8474  5003]\n",
      " [ 2545 11124  2358]\n",
      " [ 6961 10357 80309]]\n",
      "Epoch 27/30, Train Loss: 0.5824\n",
      "Validation - Accuracy: 0.7986, Precision: 0.8439, Recall: 0.7986, F1: 0.8142\n",
      "Confusion Matrix:\n",
      "[[51397  8741  4732]\n",
      " [ 2384 11509  2134]\n",
      " [ 6972 10993 79662]]\n",
      "Epoch 28/30, Train Loss: 0.5765\n",
      "Validation - Accuracy: 0.8068, Precision: 0.8440, Recall: 0.8068, F1: 0.8197\n",
      "Confusion Matrix:\n",
      "[[52629  7617  4624]\n",
      " [ 2586 11240  2201]\n",
      " [ 7482  9974 80171]]\n",
      "Epoch 29/30, Train Loss: 0.5681\n",
      "Validation - Accuracy: 0.8144, Precision: 0.8431, Recall: 0.8144, F1: 0.8246\n",
      "Confusion Matrix:\n",
      "[[53528  6569  4773]\n",
      " [ 2829 10836  2362]\n",
      " [ 7742  8861 81024]]\n",
      "Epoch 30/30, Train Loss: 0.5612\n",
      "Validation - Accuracy: 0.8145, Precision: 0.8458, Recall: 0.8145, F1: 0.8254\n",
      "Confusion Matrix:\n",
      "[[53702  6694  4474]\n",
      " [ 2687 11158  2182]\n",
      " [ 7867  9215 80545]]\n",
      "\n",
      "Training LSTMModel...\n",
      "Epoch 1/30, Train Loss: 1.0703\n",
      "Validation - Accuracy: 0.5370, Precision: 0.5554, Recall: 0.5370, F1: 0.5180\n",
      "Confusion Matrix:\n",
      "[[48644   930 15296]\n",
      " [10958   293  4776]\n",
      " [49327  1373 46927]]\n",
      "Epoch 2/30, Train Loss: 1.0125\n",
      "Validation - Accuracy: 0.6315, Precision: 0.6720, Recall: 0.6315, F1: 0.6435\n",
      "Confusion Matrix:\n",
      "[[47654  7296  9920]\n",
      " [ 8529  2701  4797]\n",
      " [23052 12184 62391]]\n",
      "Epoch 3/30, Train Loss: 0.9066\n",
      "Validation - Accuracy: 0.6674, Precision: 0.7312, Recall: 0.6674, F1: 0.6944\n",
      "Confusion Matrix:\n",
      "[[43711 12735  8424]\n",
      " [ 6066  4774  5187]\n",
      " [11385 15584 70658]]\n",
      "Epoch 4/30, Train Loss: 0.8552\n",
      "Validation - Accuracy: 0.6648, Precision: 0.7630, Recall: 0.6648, F1: 0.7028\n",
      "Confusion Matrix:\n",
      "[[45290 14102  5478]\n",
      " [ 5916  6233  3878]\n",
      " [ 9890 20575 67162]]\n",
      "Epoch 5/30, Train Loss: 0.8246\n",
      "Validation - Accuracy: 0.6766, Precision: 0.7797, Recall: 0.6766, F1: 0.7167\n",
      "Confusion Matrix:\n",
      "[[44145 15775  4950]\n",
      " [ 5172  6817  4038]\n",
      " [ 7791 20010 69826]]\n",
      "Epoch 6/30, Train Loss: 0.8012\n",
      "Validation - Accuracy: 0.6827, Precision: 0.7907, Recall: 0.6827, F1: 0.7237\n",
      "Confusion Matrix:\n",
      "[[45961 14845  4064]\n",
      " [ 5382  7165  3480]\n",
      " [ 7660 21206 68761]]\n",
      "Epoch 7/30, Train Loss: 0.7829\n",
      "Validation - Accuracy: 0.6754, Precision: 0.8058, Recall: 0.6754, F1: 0.7233\n",
      "Confusion Matrix:\n",
      "[[42904 18059  3907]\n",
      " [ 4264  8347  3416]\n",
      " [ 5506 22804 69317]]\n",
      "Epoch 8/30, Train Loss: 0.7668\n",
      "Validation - Accuracy: 0.6731, Precision: 0.8143, Recall: 0.6731, F1: 0.7235\n",
      "Confusion Matrix:\n",
      "[[44463 17383  3024]\n",
      " [ 4446  8779  2802]\n",
      " [ 5707 24991 66929]]\n",
      "Epoch 9/30, Train Loss: 0.7532\n",
      "Validation - Accuracy: 0.6875, Precision: 0.8155, Recall: 0.6875, F1: 0.7342\n",
      "Confusion Matrix:\n",
      "[[44687 16982  3201]\n",
      " [ 4356  8612  3059]\n",
      " [ 5476 22722 69429]]\n",
      "Epoch 10/30, Train Loss: 0.7410\n",
      "Validation - Accuracy: 0.6874, Precision: 0.8238, Recall: 0.6874, F1: 0.7361\n",
      "Confusion Matrix:\n",
      "[[42262 19282  3326]\n",
      " [ 3550  9255  3222]\n",
      " [ 4275 22157 71195]]\n",
      "Epoch 11/30, Train Loss: 0.7296\n",
      "Validation - Accuracy: 0.6943, Precision: 0.8211, Recall: 0.6943, F1: 0.7397\n",
      "Confusion Matrix:\n",
      "[[46905 15247  2718]\n",
      " [ 4593  8829  2605]\n",
      " [ 5856 23559 68212]]\n",
      "Epoch 12/30, Train Loss: 0.7193\n",
      "Validation - Accuracy: 0.7000, Precision: 0.8247, Recall: 0.7000, F1: 0.7449\n",
      "Confusion Matrix:\n",
      "[[46503 15713  2654]\n",
      " [ 4325  9003  2699]\n",
      " [ 5515 22651 69461]]\n",
      "Epoch 13/30, Train Loss: 0.7090\n",
      "Validation - Accuracy: 0.6978, Precision: 0.8336, Recall: 0.6978, F1: 0.7455\n",
      "Confusion Matrix:\n",
      "[[43138 18669  3063]\n",
      " [ 3266  9872  2889]\n",
      " [ 3967 22094 71566]]\n",
      "Epoch 14/30, Train Loss: 0.6993\n",
      "Validation - Accuracy: 0.6892, Precision: 0.8386, Recall: 0.6892, F1: 0.7404\n",
      "Confusion Matrix:\n",
      "[[43720 18745  2405]\n",
      " [ 3301 10376  2350]\n",
      " [ 4287 24397 68943]]\n",
      "Epoch 15/30, Train Loss: 0.6899\n",
      "Validation - Accuracy: 0.6994, Precision: 0.8385, Recall: 0.6994, F1: 0.7475\n",
      "Confusion Matrix:\n",
      "[[45546 16996  2328]\n",
      " [ 3583 10161  2283]\n",
      " [ 4620 23857 69150]]\n",
      "Epoch 16/30, Train Loss: 0.6809\n",
      "Validation - Accuracy: 0.7096, Precision: 0.8371, Recall: 0.7096, F1: 0.7530\n",
      "Confusion Matrix:\n",
      "[[40813 20077  3980]\n",
      " [ 2391 10092  3544]\n",
      " [ 2978 18868 75781]]\n",
      "Epoch 17/30, Train Loss: 0.6722\n",
      "Validation - Accuracy: 0.7135, Precision: 0.8404, Recall: 0.7135, F1: 0.7578\n",
      "Confusion Matrix:\n",
      "[[43952 17857  3061]\n",
      " [ 2940 10282  2805]\n",
      " [ 3764 20722 73141]]\n",
      "Epoch 18/30, Train Loss: 0.6634\n",
      "Validation - Accuracy: 0.7124, Precision: 0.8431, Recall: 0.7124, F1: 0.7576\n",
      "Confusion Matrix:\n",
      "[[45255 16976  2639]\n",
      " [ 3078 10561  2388]\n",
      " [ 4184 22078 71365]]\n",
      "Epoch 19/30, Train Loss: 0.6547\n",
      "Validation - Accuracy: 0.7134, Precision: 0.8472, Recall: 0.7134, F1: 0.7591\n",
      "Confusion Matrix:\n",
      "[[44846 17442  2582]\n",
      " [ 2880 10903  2244]\n",
      " [ 3933 22092 71602]]\n",
      "Epoch 20/30, Train Loss: 0.6461\n",
      "Validation - Accuracy: 0.7169, Precision: 0.8475, Recall: 0.7169, F1: 0.7612\n",
      "Confusion Matrix:\n",
      "[[43631 18280  2959]\n",
      " [ 2539 11019  2469]\n",
      " [ 3601 20701 73325]]\n",
      "Epoch 21/30, Train Loss: 0.6375\n",
      "Validation - Accuracy: 0.7231, Precision: 0.8486, Recall: 0.7231, F1: 0.7658\n",
      "Confusion Matrix:\n",
      "[[43689 18065  3116]\n",
      " [ 2388 10979  2660]\n",
      " [ 3388 19811 74428]]\n",
      "Epoch 22/30, Train Loss: 0.6296\n",
      "Validation - Accuracy: 0.7198, Precision: 0.8498, Recall: 0.7198, F1: 0.7637\n",
      "Confusion Matrix:\n",
      "[[46318 16178  2374]\n",
      " [ 2826 11238  1963]\n",
      " [ 4432 22246 70949]]\n",
      "Epoch 23/30, Train Loss: 0.6208\n",
      "Validation - Accuracy: 0.7255, Precision: 0.8519, Recall: 0.7255, F1: 0.7682\n",
      "Confusion Matrix:\n",
      "[[45072 17088  2710]\n",
      " [ 2477 11342  2208]\n",
      " [ 3824 20704 73099]]\n",
      "Epoch 24/30, Train Loss: 0.6126\n",
      "Validation - Accuracy: 0.7336, Precision: 0.8513, Recall: 0.7336, F1: 0.7737\n",
      "Confusion Matrix:\n",
      "[[45401 16519  2950]\n",
      " [ 2426 11225  2376]\n",
      " [ 3783 19509 74335]]\n",
      "Epoch 25/30, Train Loss: 0.6045\n",
      "Validation - Accuracy: 0.7322, Precision: 0.8545, Recall: 0.7322, F1: 0.7731\n",
      "Confusion Matrix:\n",
      "[[44329 17486  3055]\n",
      " [ 2081 11578  2368]\n",
      " [ 3441 19381 74805]]\n",
      "Epoch 26/30, Train Loss: 0.5963\n",
      "Validation - Accuracy: 0.7448, Precision: 0.8494, Recall: 0.7448, F1: 0.7809\n",
      "Confusion Matrix:\n",
      "[[47805 14259  2806]\n",
      " [ 2837 11032  2158]\n",
      " [ 4601 18896 74130]]\n",
      "Epoch 27/30, Train Loss: 0.5889\n",
      "Validation - Accuracy: 0.7362, Precision: 0.8545, Recall: 0.7362, F1: 0.7757\n",
      "Confusion Matrix:\n",
      "[[47463 14953  2454]\n",
      " [ 2618 11669  1740]\n",
      " [ 4589 20744 72294]]\n",
      "Epoch 28/30, Train Loss: 0.5808\n",
      "Validation - Accuracy: 0.7490, Precision: 0.8527, Recall: 0.7490, F1: 0.7844\n",
      "Confusion Matrix:\n",
      "[[47218 14684  2968]\n",
      " [ 2449 11405  2173]\n",
      " [ 4323 18219 75085]]\n",
      "Epoch 29/30, Train Loss: 0.5736\n",
      "Validation - Accuracy: 0.7495, Precision: 0.8556, Recall: 0.7495, F1: 0.7852\n",
      "Confusion Matrix:\n",
      "[[46380 15364  3126]\n",
      " [ 2126 11682  2219]\n",
      " [ 3922 17964 75741]]\n",
      "Epoch 30/30, Train Loss: 0.5659\n",
      "Validation - Accuracy: 0.7565, Precision: 0.8537, Recall: 0.7565, F1: 0.7895\n",
      "Confusion Matrix:\n",
      "[[48620 13389  2861]\n",
      " [ 2571 11532  1924]\n",
      " [ 4797 17927 74903]]\n",
      "\n",
      "Training BiLSTMModel...\n",
      "Epoch 1/30, Train Loss: 1.0156\n",
      "Validation - Accuracy: 0.6360, Precision: 0.6980, Recall: 0.6360, F1: 0.6540\n",
      "Confusion Matrix:\n",
      "[[48965  8639  7266]\n",
      " [ 8107  3936  3984]\n",
      " [22488 14495 60644]]\n",
      "Epoch 2/30, Train Loss: 0.8872\n",
      "Validation - Accuracy: 0.6706, Precision: 0.7553, Recall: 0.6706, F1: 0.7023\n",
      "Confusion Matrix:\n",
      "[[46095 12703  6072]\n",
      " [ 5734  6711  3582]\n",
      " [12220 18487 66920]]\n",
      "Epoch 3/30, Train Loss: 0.8304\n",
      "Validation - Accuracy: 0.6717, Precision: 0.7813, Recall: 0.6717, F1: 0.7103\n",
      "Confusion Matrix:\n",
      "[[46183 14308  4379]\n",
      " [ 5031  8208  2788]\n",
      " [10563 21536 65528]]\n",
      "Epoch 4/30, Train Loss: 0.7929\n",
      "Validation - Accuracy: 0.6833, Precision: 0.7953, Recall: 0.6833, F1: 0.7229\n",
      "Confusion Matrix:\n",
      "[[45533 15133  4204]\n",
      " [ 4386  8934  2707]\n",
      " [ 8861 21252 67514]]\n",
      "Epoch 5/30, Train Loss: 0.7659\n",
      "Validation - Accuracy: 0.6892, Precision: 0.8052, Recall: 0.6892, F1: 0.7297\n",
      "Confusion Matrix:\n",
      "[[45670 15438  3762]\n",
      " [ 4086  9470  2471]\n",
      " [ 8262 21472 67893]]\n",
      "Epoch 6/30, Train Loss: 0.7442\n",
      "Validation - Accuracy: 0.7020, Precision: 0.8063, Recall: 0.7020, F1: 0.7386\n",
      "Confusion Matrix:\n",
      "[[47587 13421  3862]\n",
      " [ 4355  9210  2462]\n",
      " [ 8386 20722 68519]]\n",
      "Epoch 7/30, Train Loss: 0.7260\n",
      "Validation - Accuracy: 0.7015, Precision: 0.8169, Recall: 0.7015, F1: 0.7411\n",
      "Confusion Matrix:\n",
      "[[46682 14672  3516]\n",
      " [ 3857 10033  2137]\n",
      " [ 7490 21624 68513]]\n",
      "Epoch 8/30, Train Loss: 0.7089\n",
      "Validation - Accuracy: 0.7238, Precision: 0.8140, Recall: 0.7238, F1: 0.7549\n",
      "Confusion Matrix:\n",
      "[[50020 11514  3336]\n",
      " [ 4592  9291  2144]\n",
      " [ 8983 18740 69904]]\n",
      "Epoch 9/30, Train Loss: 0.6928\n",
      "Validation - Accuracy: 0.7225, Precision: 0.8252, Recall: 0.7225, F1: 0.7583\n",
      "Confusion Matrix:\n",
      "[[44260 16066  4544]\n",
      " [ 2891 10518  2618]\n",
      " [ 5546 17867 74214]]\n",
      "Epoch 10/30, Train Loss: 0.6787\n",
      "Validation - Accuracy: 0.7294, Precision: 0.8243, Recall: 0.7294, F1: 0.7624\n",
      "Confusion Matrix:\n",
      "[[48297 13188  3385]\n",
      " [ 3705 10197  2125]\n",
      " [ 7580 18318 71729]]\n",
      "Epoch 11/30, Train Loss: 0.6649\n",
      "Validation - Accuracy: 0.7316, Precision: 0.8310, Recall: 0.7316, F1: 0.7659\n",
      "Confusion Matrix:\n",
      "[[46006 14951  3913]\n",
      " [ 2894 10836  2297]\n",
      " [ 6043 17819 73765]]\n",
      "Epoch 12/30, Train Loss: 0.6517\n",
      "Validation - Accuracy: 0.7323, Precision: 0.8293, Recall: 0.7323, F1: 0.7647\n",
      "Confusion Matrix:\n",
      "[[49645 12367  2858]\n",
      " [ 3728 10559  1740]\n",
      " [ 8139 18960 70528]]\n",
      "Epoch 13/30, Train Loss: 0.6393\n",
      "Validation - Accuracy: 0.7357, Precision: 0.8341, Recall: 0.7357, F1: 0.7683\n",
      "Confusion Matrix:\n",
      "[[49649 12500  2721]\n",
      " [ 3516 10915  1596]\n",
      " [ 7872 18980 70775]]\n",
      "Epoch 14/30, Train Loss: 0.6268\n",
      "Validation - Accuracy: 0.7424, Precision: 0.8385, Recall: 0.7424, F1: 0.7749\n",
      "Confusion Matrix:\n",
      "[[48037 13577  3256]\n",
      " [ 2962 11302  1763]\n",
      " [ 6619 17808 73200]]\n",
      "Epoch 15/30, Train Loss: 0.6143\n",
      "Validation - Accuracy: 0.7377, Precision: 0.8339, Recall: 0.7377, F1: 0.7685\n",
      "Confusion Matrix:\n",
      "[[51728 10654  2488]\n",
      " [ 3983 10670  1374]\n",
      " [ 8521 19803 69303]]\n",
      "Epoch 16/30, Train Loss: 0.6017\n",
      "Validation - Accuracy: 0.7561, Precision: 0.8411, Recall: 0.7561, F1: 0.7849\n",
      "Confusion Matrix:\n",
      "[[48836 12551  3483]\n",
      " [ 2874 11384  1769]\n",
      " [ 6505 16363 74759]]\n",
      "Epoch 17/30, Train Loss: 0.5900\n",
      "Validation - Accuracy: 0.7548, Precision: 0.8449, Recall: 0.7548, F1: 0.7850\n",
      "Confusion Matrix:\n",
      "[[46709 14342  3819]\n",
      " [ 2238 11804  1985]\n",
      " [ 5547 15848 76232]]\n",
      "Epoch 18/30, Train Loss: 0.5790\n",
      "Validation - Accuracy: 0.7709, Precision: 0.8431, Recall: 0.7709, F1: 0.7957\n",
      "Confusion Matrix:\n",
      "[[49086 11763  4021]\n",
      " [ 2664 11379  1984]\n",
      " [ 6052 14421 77154]]\n",
      "Epoch 19/30, Train Loss: 0.5674\n",
      "Validation - Accuracy: 0.7538, Precision: 0.8472, Recall: 0.7538, F1: 0.7841\n",
      "Confusion Matrix:\n",
      "[[49703 12289  2878]\n",
      " [ 2649 12025  1353]\n",
      " [ 6916 17866 72845]]\n",
      "Epoch 20/30, Train Loss: 0.5562\n",
      "Validation - Accuracy: 0.7701, Precision: 0.8480, Recall: 0.7701, F1: 0.7961\n",
      "Confusion Matrix:\n",
      "[[49956 11262  3652]\n",
      " [ 2623 11880  1524]\n",
      " [ 6215 15758 75654]]\n",
      "Epoch 21/30, Train Loss: 0.5457\n",
      "Validation - Accuracy: 0.7709, Precision: 0.8503, Recall: 0.7709, F1: 0.7970\n",
      "Confusion Matrix:\n",
      "[[50783 10855  3232]\n",
      " [ 2670 11957  1400]\n",
      " [ 6482 16269 74876]]\n",
      "Epoch 22/30, Train Loss: 0.5358\n",
      "Validation - Accuracy: 0.7841, Precision: 0.8461, Recall: 0.7841, F1: 0.8043\n",
      "Confusion Matrix:\n",
      "[[52719  9017  3134]\n",
      " [ 3061 11554  1412]\n",
      " [ 8061 13857 75709]]\n",
      "Epoch 23/30, Train Loss: 0.5254\n",
      "Validation - Accuracy: 0.7917, Precision: 0.8484, Recall: 0.7917, F1: 0.8109\n",
      "Confusion Matrix:\n",
      "[[50027 10148  4695]\n",
      " [ 2222 11863  1942]\n",
      " [ 5897 12283 79447]]\n",
      "Epoch 24/30, Train Loss: 0.5148\n",
      "Validation - Accuracy: 0.7755, Precision: 0.8503, Recall: 0.7755, F1: 0.7994\n",
      "Confusion Matrix:\n",
      "[[52181  9900  2789]\n",
      " [ 2774 11989  1264]\n",
      " [ 7584 15774 74269]]\n",
      "Epoch 25/30, Train Loss: 0.5062\n",
      "Validation - Accuracy: 0.7910, Precision: 0.8540, Recall: 0.7910, F1: 0.8117\n",
      "Confusion Matrix:\n",
      "[[50745 10304  3821]\n",
      " [ 2177 12352  1498]\n",
      " [ 6351 13159 78117]]\n",
      "Epoch 26/30, Train Loss: 0.4957\n",
      "Validation - Accuracy: 0.7793, Precision: 0.8575, Recall: 0.7793, F1: 0.8043\n",
      "Confusion Matrix:\n",
      "[[50906 10896  3068]\n",
      " [ 2136 12688  1203]\n",
      " [ 6456 15634 75537]]\n",
      "Epoch 27/30, Train Loss: 0.4870\n",
      "Validation - Accuracy: 0.7923, Precision: 0.8560, Recall: 0.7923, F1: 0.8129\n",
      "Confusion Matrix:\n",
      "[[51550  9707  3613]\n",
      " [ 2211 12525  1291]\n",
      " [ 6573 13688 77366]]\n",
      "Epoch 28/30, Train Loss: 0.4780\n",
      "Validation - Accuracy: 0.8042, Precision: 0.8545, Recall: 0.8042, F1: 0.8208\n",
      "Confusion Matrix:\n",
      "[[51557  8979  4334]\n",
      " [ 2138 12295  1594]\n",
      " [ 6388 11521 79718]]\n",
      "Epoch 29/30, Train Loss: 0.4694\n",
      "Validation - Accuracy: 0.7942, Precision: 0.8483, Recall: 0.7942, F1: 0.8106\n",
      "Confusion Matrix:\n",
      "[[55076  7104  2690]\n",
      " [ 3426 11492  1109]\n",
      " [ 9302 13110 75215]]\n",
      "Epoch 30/30, Train Loss: 0.4618\n",
      "Validation - Accuracy: 0.8027, Precision: 0.8569, Recall: 0.8027, F1: 0.8203\n",
      "Confusion Matrix:\n",
      "[[49677 10548  4645]\n",
      " [ 1592 12618  1817]\n",
      " [ 5612 11014 81001]]\n",
      "\n",
      "Training GRUModel...\n",
      "Epoch 1/30, Train Loss: 0.9584\n",
      "Validation - Accuracy: 0.6653, Precision: 0.7114, Recall: 0.6653, F1: 0.6850\n",
      "Confusion Matrix:\n",
      "[[42235 11303 11332]\n",
      " [ 5387  5117  5523]\n",
      " [12907 13299 71421]]\n",
      "Epoch 2/30, Train Loss: 0.8588\n",
      "Validation - Accuracy: 0.6774, Precision: 0.7492, Recall: 0.6774, F1: 0.7059\n",
      "Confusion Matrix:\n",
      "[[42989 13434  8447]\n",
      " [ 4728  6802  4497]\n",
      " [10172 16317 71138]]\n",
      "Epoch 3/30, Train Loss: 0.8152\n",
      "Validation - Accuracy: 0.6982, Precision: 0.7640, Recall: 0.6982, F1: 0.7240\n",
      "Confusion Matrix:\n",
      "[[46612 11435  6823]\n",
      " [ 5172  6858  3997]\n",
      " [10667 15786 71174]]\n",
      "Epoch 4/30, Train Loss: 0.7849\n",
      "Validation - Accuracy: 0.7135, Precision: 0.7748, Recall: 0.7135, F1: 0.7378\n",
      "Confusion Matrix:\n",
      "[[46790 11238  6842]\n",
      " [ 4865  7172  3990]\n",
      " [ 9305 14902 73420]]\n",
      "Epoch 5/30, Train Loss: 0.7600\n",
      "Validation - Accuracy: 0.7135, Precision: 0.7873, Recall: 0.7135, F1: 0.7413\n",
      "Confusion Matrix:\n",
      "[[47490 11939  5441]\n",
      " [ 4620  8051  3356]\n",
      " [ 9483 16301 71843]]\n",
      "Epoch 6/30, Train Loss: 0.7393\n",
      "Validation - Accuracy: 0.7153, Precision: 0.7971, Recall: 0.7153, F1: 0.7454\n",
      "Confusion Matrix:\n",
      "[[47444 12513  4913]\n",
      " [ 4364  8668  2995]\n",
      " [ 8810 17222 71595]]\n",
      "Epoch 7/30, Train Loss: 0.7206\n",
      "Validation - Accuracy: 0.7195, Precision: 0.8041, Recall: 0.7195, F1: 0.7501\n",
      "Confusion Matrix:\n",
      "[[47874 12505  4491]\n",
      " [ 4230  9066  2731]\n",
      " [ 8590 17522 71515]]\n",
      "Epoch 8/30, Train Loss: 0.7042\n",
      "Validation - Accuracy: 0.7242, Precision: 0.8105, Recall: 0.7242, F1: 0.7555\n",
      "Confusion Matrix:\n",
      "[[46798 13359  4713]\n",
      " [ 3763  9475  2789]\n",
      " [ 7408 17208 73011]]\n",
      "Epoch 9/30, Train Loss: 0.6891\n",
      "Validation - Accuracy: 0.7171, Precision: 0.8180, Recall: 0.7171, F1: 0.7525\n",
      "Confusion Matrix:\n",
      "[[45825 14833  4212]\n",
      " [ 3327 10195  2505]\n",
      " [ 6929 18707 71991]]\n",
      "Epoch 10/30, Train Loss: 0.6752\n",
      "Validation - Accuracy: 0.7252, Precision: 0.8206, Recall: 0.7252, F1: 0.7587\n",
      "Confusion Matrix:\n",
      "[[46837 13962  4071]\n",
      " [ 3400 10239  2388]\n",
      " [ 7091 18145 72391]]\n",
      "Epoch 11/30, Train Loss: 0.6614\n",
      "Validation - Accuracy: 0.7278, Precision: 0.8180, Recall: 0.7278, F1: 0.7575\n",
      "Confusion Matrix:\n",
      "[[50646 11383  2841]\n",
      " [ 4288  9861  1878]\n",
      " [ 9974 18231 69422]]\n",
      "Epoch 12/30, Train Loss: 0.6489\n",
      "Validation - Accuracy: 0.7485, Precision: 0.8184, Recall: 0.7485, F1: 0.7731\n",
      "Confusion Matrix:\n",
      "[[50757 10423  3690]\n",
      " [ 4177  9487  2363]\n",
      " [ 8856 15398 73373]]\n",
      "Epoch 13/30, Train Loss: 0.6368\n",
      "Validation - Accuracy: 0.7314, Precision: 0.8302, Recall: 0.7314, F1: 0.7651\n",
      "Confusion Matrix:\n",
      "[[47693 13646  3531]\n",
      " [ 3173 10880  1974]\n",
      " [ 6981 18653 71993]]\n",
      "Epoch 14/30, Train Loss: 0.6246\n",
      "Validation - Accuracy: 0.7443, Precision: 0.8315, Recall: 0.7443, F1: 0.7747\n",
      "Confusion Matrix:\n",
      "[[47208 13544  4118]\n",
      " [ 2886 10827  2314]\n",
      " [ 6305 16473 74849]]\n",
      "Epoch 15/30, Train Loss: 0.6134\n",
      "Validation - Accuracy: 0.7593, Precision: 0.8285, Recall: 0.7593, F1: 0.7838\n",
      "Confusion Matrix:\n",
      "[[49948 10997  3925]\n",
      " [ 3466 10223  2338]\n",
      " [ 7480 14773 75374]]\n",
      "Epoch 16/30, Train Loss: 0.6017\n",
      "Validation - Accuracy: 0.7660, Precision: 0.8286, Recall: 0.7660, F1: 0.7886\n",
      "Confusion Matrix:\n",
      "[[49473 10816  4581]\n",
      " [ 3232 10232  2563]\n",
      " [ 6881 13696 77050]]\n",
      "Epoch 17/30, Train Loss: 0.5904\n",
      "Validation - Accuracy: 0.7588, Precision: 0.8298, Recall: 0.7588, F1: 0.7822\n",
      "Confusion Matrix:\n",
      "[[52585  9354  2931]\n",
      " [ 4029 10261  1737]\n",
      " [ 9301 15701 72625]]\n",
      "Epoch 18/30, Train Loss: 0.5799\n",
      "Validation - Accuracy: 0.7432, Precision: 0.8347, Recall: 0.7432, F1: 0.7717\n",
      "Confusion Matrix:\n",
      "[[52125 10515  2230]\n",
      " [ 3655 11007  1365]\n",
      " [ 9522 18559 69546]]\n",
      "Epoch 19/30, Train Loss: 0.5679\n",
      "Validation - Accuracy: 0.7592, Precision: 0.8420, Recall: 0.7592, F1: 0.7873\n",
      "Confusion Matrix:\n",
      "[[47070 13355  4445]\n",
      " [ 2310 11588  2129]\n",
      " [ 5368 15376 76883]]\n",
      "Epoch 20/30, Train Loss: 0.5579\n",
      "Validation - Accuracy: 0.7716, Precision: 0.8392, Recall: 0.7716, F1: 0.7951\n",
      "Confusion Matrix:\n",
      "[[48484 11704  4682]\n",
      " [ 2500 11207  2320]\n",
      " [ 5860 13704 78063]]\n",
      "Epoch 21/30, Train Loss: 0.5478\n",
      "Validation - Accuracy: 0.7801, Precision: 0.8370, Recall: 0.7801, F1: 0.7997\n",
      "Confusion Matrix:\n",
      "[[52206  9113  3551]\n",
      " [ 3406 10650  1971]\n",
      " [ 8064 13146 76417]]\n",
      "Epoch 22/30, Train Loss: 0.5369\n",
      "Validation - Accuracy: 0.7806, Precision: 0.8389, Recall: 0.7806, F1: 0.8001\n",
      "Confusion Matrix:\n",
      "[[52809  8887  3174]\n",
      " [ 3404 10835  1788]\n",
      " [ 8540 13379 75708]]\n",
      "Epoch 23/30, Train Loss: 0.5265\n",
      "Validation - Accuracy: 0.7852, Precision: 0.8424, Recall: 0.7852, F1: 0.8047\n",
      "Confusion Matrix:\n",
      "[[51850  9501  3519]\n",
      " [ 2961 11144  1922]\n",
      " [ 7650 12796 77181]]\n",
      "Epoch 24/30, Train Loss: 0.5168\n",
      "Validation - Accuracy: 0.7867, Precision: 0.8459, Recall: 0.7867, F1: 0.8067\n",
      "Confusion Matrix:\n",
      "[[52170  9172  3528]\n",
      " [ 2950 11313  1764]\n",
      " [ 7178 13481 76968]]\n",
      "Epoch 25/30, Train Loss: 0.5068\n",
      "Validation - Accuracy: 0.7910, Precision: 0.8446, Recall: 0.7910, F1: 0.8087\n",
      "Confusion Matrix:\n",
      "[[53515  8168  3187]\n",
      " [ 3286 11093  1648]\n",
      " [ 8202 12823 76602]]\n",
      "Epoch 26/30, Train Loss: 0.4975\n",
      "Validation - Accuracy: 0.7976, Precision: 0.8478, Recall: 0.7976, F1: 0.8149\n",
      "Confusion Matrix:\n",
      "[[51316  9264  4290]\n",
      " [ 2491 11521  2015]\n",
      " [ 6524 11544 79559]]\n",
      "Epoch 27/30, Train Loss: 0.4878\n",
      "Validation - Accuracy: 0.7837, Precision: 0.8535, Recall: 0.7837, F1: 0.8063\n",
      "Confusion Matrix:\n",
      "[[51698 10145  3027]\n",
      " [ 2371 12255  1401]\n",
      " [ 7135 14530 75962]]\n",
      "Epoch 28/30, Train Loss: 0.4783\n",
      "Validation - Accuracy: 0.7944, Precision: 0.8476, Recall: 0.7944, F1: 0.8112\n",
      "Confusion Matrix:\n",
      "[[54660  7447  2763]\n",
      " [ 3318 11320  1389]\n",
      " [ 8905 12874 75848]]\n",
      "Epoch 29/30, Train Loss: 0.4696\n",
      "Validation - Accuracy: 0.8052, Precision: 0.8535, Recall: 0.8052, F1: 0.8215\n",
      "Confusion Matrix:\n",
      "[[52181  8727  3962]\n",
      " [ 2362 11873  1792]\n",
      " [ 6610 11319 79698]]\n",
      "Epoch 30/30, Train Loss: 0.4610\n",
      "Validation - Accuracy: 0.7992, Precision: 0.8575, Recall: 0.7992, F1: 0.8182\n",
      "Confusion Matrix:\n",
      "[[51812  9568  3490]\n",
      " [ 2124 12396  1507]\n",
      " [ 6570 12594 78463]]\n"
     ]
    }
   ],
   "source": [
    "    # Train individual models\n",
    "    for idx, model in enumerate(models):\n",
    "        model_name = model.__class__.__name__\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss = train_model(model, train_loader, optimizer, criterion)\n",
    "            accuracy, precision, recall, f1, conf_matrix = evaluate_model([model], val_loader, is_ensemble=False)\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09edab06-13cc-4a3b-bd94-05b4d0ca0fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Ensemble...\n",
      "Ensemble - Accuracy: 0.8290, Precision: 0.8729, Recall: 0.8290, F1: 0.8432\n",
      "Confusion Matrix:\n",
      "[[53564  8105  3201]\n",
      " [ 1694 12832  1501]\n",
      " [ 5775 10259 81593]]\n"
     ]
    }
   ],
   "source": [
    "    # Evaluate ensemble\n",
    "    print(\"\\nEvaluating Ensemble...\")\n",
    "    accuracy, precision, recall, f1, conf_matrix = evaluate_model(models, val_loader, is_ensemble=True)\n",
    "    print(f\"Ensemble - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4cd887a-886c-4966-8d7e-dbb636f43cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Save models\n",
    "    with open(\"ensemble_absa_models_V1.pkl\", \"wb\") as f:\n",
    "        pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9965098-9530-446b-854c-eb9ee5c090a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting aspects: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 332.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Aspects: ['usability']\n",
      "\n",
      "Custom Text: The software is very user-friendly and fast, but the customer support is terrible and it's overpriced.\n",
      "Predicted Sentiments:\n",
      "Aspect: usability, Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "    # Example prediction\n",
    "    custom_text = \"The software is very user-friendly and fast, but the customer support is terrible and it's overpriced.\"\n",
    "\n",
    "    # Extract aspects\n",
    "    aspects = cluster_aspects_for_text(custom_text, vectorizer, kmeans)\n",
    "    print(f\"Extracted Aspects: {aspects}\")\n",
    "    \n",
    "    # Predict sentiments\n",
    "    predicted_sentiments = predict_text_absa(custom_text, aspects, TOKENIZER, models)\n",
    "    print(f\"\\nCustom Text: {custom_text}\")\n",
    "    print(\"Predicted Sentiments:\")\n",
    "    for aspect, sentiment in predicted_sentiments.items():\n",
    "        print(f\"Aspect: {aspect}, Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0959f669-6229-4ef1-b6df-4f9b5ec917fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame head after aspect extraction and clustering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>extracted_aspects</th>\n",
       "      <th>final_aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Strong backgroung, good read, quite up to date...</td>\n",
       "      <td>strong backgroung good read quite date takes h...</td>\n",
       "      <td>[(quite date, usability), (holistic approach s...</td>\n",
       "      <td>[usability]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>If you live on Mars and never heard of the int...</td>\n",
       "      <td>live mars never heard internet good book</td>\n",
       "      <td>[(internet good book, new_good)]</td>\n",
       "      <td>[new_good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>i got this book on amazon and it ended up savi...</td>\n",
       "      <td>got book amazon ended saving lot money great c...</td>\n",
       "      <td>[(book amazon, new_amazon), (lot money great c...</td>\n",
       "      <td>[new_amazon, new_great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>I was very happy with this purchase because th...</td>\n",
       "      <td>happy purchase shipment super fast thanks</td>\n",
       "      <td>[(happy purchase shipment super fast thanks, p...</td>\n",
       "      <td>[usability]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>Recieved in a timely manner- book in great con...</td>\n",
       "      <td>recieved timely manner book great condition ma...</td>\n",
       "      <td>[(timely manner book great condition, new_grea...</td>\n",
       "      <td>[new_great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375081</th>\n",
       "      <td>457815</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>My son wanted this game and was so excited to ...</td>\n",
       "      <td>son wanted game excited buy money work three c...</td>\n",
       "      <td>[(money, new_one), (money, new_money), (three ...</td>\n",
       "      <td>[usage_context, new_money, usability]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375082</th>\n",
       "      <td>457879</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>I bought this CD because I am an international...</td>\n",
       "      <td>bought cd international student us like much f...</td>\n",
       "      <td>[(though still greatly pleased care concern, u...</td>\n",
       "      <td>[usability]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375083</th>\n",
       "      <td>457880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>I bought this program and like it.  The gentle...</td>\n",
       "      <td>bought program like gentleman said greatly dis...</td>\n",
       "      <td>[(program, device_requirements), (program, new...</td>\n",
       "      <td>[new_program, usability]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375084</th>\n",
       "      <td>457881</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>I was very dissapointed with this product. It ...</td>\n",
       "      <td>dissapointed product much except nice interfac...</td>\n",
       "      <td>[(dissapointed product, new_product), (nice in...</td>\n",
       "      <td>[usability, new_product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375085</th>\n",
       "      <td>457982</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>Purchased this because it was supposed to work...</td>\n",
       "      <td>purchased supposed work netgear fvs vpn router...</td>\n",
       "      <td>[(purchased supposed work netgear fvs vpn rout...</td>\n",
       "      <td>[new_software, usability]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360261 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  overall  reviewTime  \\\n",
       "0                5      1.0        2011   \n",
       "1                6      0.0        2010   \n",
       "2                7      1.0        2010   \n",
       "3                8      1.0        2010   \n",
       "4                9      1.0        2010   \n",
       "...            ...      ...         ...   \n",
       "375081      457815     -1.0        2006   \n",
       "375082      457879      1.0        2006   \n",
       "375083      457880      0.0        2002   \n",
       "375084      457881     -1.0        2002   \n",
       "375085      457982     -1.0        2004   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       Strong backgroung, good read, quite up to date...   \n",
       "1       If you live on Mars and never heard of the int...   \n",
       "2       i got this book on amazon and it ended up savi...   \n",
       "3       I was very happy with this purchase because th...   \n",
       "4       Recieved in a timely manner- book in great con...   \n",
       "...                                                   ...   \n",
       "375081  My son wanted this game and was so excited to ...   \n",
       "375082  I bought this CD because I am an international...   \n",
       "375083  I bought this program and like it.  The gentle...   \n",
       "375084  I was very dissapointed with this product. It ...   \n",
       "375085  Purchased this because it was supposed to work...   \n",
       "\n",
       "                                             cleaned_text  \\\n",
       "0       strong backgroung good read quite date takes h...   \n",
       "1                live mars never heard internet good book   \n",
       "2       got book amazon ended saving lot money great c...   \n",
       "3               happy purchase shipment super fast thanks   \n",
       "4       recieved timely manner book great condition ma...   \n",
       "...                                                   ...   \n",
       "375081  son wanted game excited buy money work three c...   \n",
       "375082  bought cd international student us like much f...   \n",
       "375083  bought program like gentleman said greatly dis...   \n",
       "375084  dissapointed product much except nice interfac...   \n",
       "375085  purchased supposed work netgear fvs vpn router...   \n",
       "\n",
       "                                        extracted_aspects  \\\n",
       "0       [(quite date, usability), (holistic approach s...   \n",
       "1                        [(internet good book, new_good)]   \n",
       "2       [(book amazon, new_amazon), (lot money great c...   \n",
       "3       [(happy purchase shipment super fast thanks, p...   \n",
       "4       [(timely manner book great condition, new_grea...   \n",
       "...                                                   ...   \n",
       "375081  [(money, new_one), (money, new_money), (three ...   \n",
       "375082  [(though still greatly pleased care concern, u...   \n",
       "375083  [(program, device_requirements), (program, new...   \n",
       "375084  [(dissapointed product, new_product), (nice in...   \n",
       "375085  [(purchased supposed work netgear fvs vpn rout...   \n",
       "\n",
       "                                final_aspects  \n",
       "0                                 [usability]  \n",
       "1                                  [new_good]  \n",
       "2                     [new_amazon, new_great]  \n",
       "3                                 [usability]  \n",
       "4                                 [new_great]  \n",
       "...                                       ...  \n",
       "375081  [usage_context, new_money, usability]  \n",
       "375082                            [usability]  \n",
       "375083               [new_program, usability]  \n",
       "375084               [usability, new_product]  \n",
       "375085              [new_software, usability]  \n",
       "\n",
       "[360261 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print(\"\\nDataFrame head after aspect extraction and clustering:\")\n",
    "    display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b495f-f1bd-4993-9651-72136f3a151a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
