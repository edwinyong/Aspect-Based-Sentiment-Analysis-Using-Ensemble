{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef903eb-4778-4633-8482-a7b8998aba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import threading\n",
    "import spacy\n",
    "from werkzeug.serving import make_server\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd19643-3966-4042-8c90-254d6cd23ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bda780-2390-44e2-9444-a308a06d6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"lemmatizer\"])\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TOKENIZER = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "VOCAB_SIZE = TOKENIZER.vocab_size  # ~50,265 for roberta-base\n",
    "\n",
    "# Aspect Seed Keywords\n",
    "with open('aspect_seeds.pkl', 'rb') as f:\n",
    "    ASPECT_SEEDS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9f07fe-0070-467f-bf5f-f3f400b7a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "    \n",
    "with open('kmeans_model.pkl', 'rb') as f:\n",
    "    kmeans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4121a939-ee3d-4507-94e6-7509b2aeffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = contractions.fix(text)   # Expand contractions\n",
    "    text = text.lower()             # Convert text to lowercase\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\d+', '', text)    # Remove numbers\n",
    "    text = re.sub(r'\\W+', ' ', text)   # Remove special characters\n",
    "    return text.strip()             # Strip leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35517dc5-e586-4654-b2b4-ef34e735a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Aspect Discovery\n",
    "def discover_new_aspects(texts, top_n=20):\n",
    "\n",
    "    noun_phrases = []\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        noun_phrases.extend([chunk.text.lower() for chunk in doc.noun_chunks])\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features=1000, ngram_range=(1, 3))\n",
    "    X = vectorizer.fit_transform(noun_phrases)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    counts = np.array(X.sum(axis=0))[0]\n",
    "    \n",
    "    existing_terms = set().union(*ASPECT_SEEDS.values())\n",
    "    new_terms = [(term, count) for term, count in zip(terms, counts) \n",
    "                if term not in existing_terms and count > 10]\n",
    "    \n",
    "    return sorted(new_terms, key=lambda x: x[1], reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c6b477-788b-4f44-bdce-9b386cb9aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, num_labels):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embed_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_labels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [torch.max_pool1d(pool, pool.size(2)).squeeze(2) for pool in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(h_n[-1])\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.bilstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        _, (h_n, _) = self.bilstm(x)\n",
    "        h_n = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        x = self.bn(h_n)\n",
    "        return self.fc(x)\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_labels, dropout=0.3):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1).cpu()\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            packed_out, _ = self.gru(packed)\n",
    "            gru_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        else:\n",
    "            gru_out, _ = self.gru(embedded)\n",
    "        pooled = torch.mean(gru_out, dim=1)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e23d9f-9f14-453b-8150-f4485ba93fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Aspect Extraction with Parallel Processing\n",
    "def extract_aspect_terms_cpu(texts, batch_size=32):\n",
    "    aspect_results = []\n",
    "    \n",
    "    # Process in batches with progress bar\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=batch_size), \n",
    "                   total=len(texts),\n",
    "                   desc=\"Extracting aspects\"):\n",
    "        terms = []\n",
    "        \n",
    "        # Extract noun phrases\n",
    "        for chunk in doc.noun_chunks:\n",
    "            term = chunk.text.lower()\n",
    "            for aspect, keywords in ASPECT_SEEDS.items():\n",
    "                if any(keyword in term for keyword in keywords):\n",
    "                    terms.append((term, aspect))\n",
    "        \n",
    "        # Extract verbs\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                for aspect, keywords in ASPECT_SEEDS.items():\n",
    "                    if any(keyword in token.text.lower() for keyword in keywords):\n",
    "                        terms.append((token.text.lower(), aspect))\n",
    "        \n",
    "        aspect_results.append(terms if terms else [(\"general\", \"general\")])\n",
    "    \n",
    "    return aspect_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1425b3a0-0905-4a9f-b1a9-20b35785a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect Clustering for Single Text\n",
    "def cluster_aspects_for_text(text, vectorizer=None, kmeans=None):\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    aspect_results = extract_aspect_terms_cpu([cleaned_text])\n",
    "    data = pd.DataFrame({'cleaned_text': [cleaned_text], 'extracted_aspects': [aspect_results[0]]})\n",
    "    \n",
    "    # Extract terms safely\n",
    "    all_terms = []\n",
    "    for sublist in data['extracted_aspects']:\n",
    "        for item in sublist:\n",
    "            if isinstance(item, tuple) and len(item) == 2 and item[0] != 'unknown':\n",
    "                all_terms.append(item[0])\n",
    "            elif item == 'general':\n",
    "                all_terms.append('general')\n",
    "    \n",
    "    if not all_terms or all(t == 'general' for t in all_terms):\n",
    "        data['final_aspects'] = [['general']]\n",
    "        return data['final_aspects'].iloc[0]\n",
    "    \n",
    "    # Initialize or use provided vectorizer\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_terms)\n",
    "    else:\n",
    "        tfidf_matrix = vectorizer.transform(all_terms)\n",
    "    \n",
    "    # Use the number of aspects in ASPECT_SEEDS as maximum clusters\n",
    "    max_aspects = len(ASPECT_SEEDS)\n",
    "    \n",
    "    # Initialize or use provided KMeans\n",
    "    if kmeans is None:\n",
    "        n_clusters = min(max_aspects, len(set(all_terms)))  # Don't exceed number of defined aspects\n",
    "        kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "    else:\n",
    "        clusters = kmeans.predict(tfidf_matrix)\n",
    "        n_clusters = min(kmeans.n_clusters, max_aspects)  # Cap at our max aspects\n",
    "    \n",
    "    # Map terms to clusters\n",
    "    term_to_cluster = defaultdict(list)\n",
    "    for term, cluster_id in zip(all_terms, clusters):\n",
    "        term_to_cluster[cluster_id].append(term)\n",
    "    \n",
    "    # Name clusters based on seed terms - ensure we cover all possible cluster IDs\n",
    "    aspect_names = {}\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_terms = term_to_cluster.get(cluster_id, [])\n",
    "        max_overlap = 0\n",
    "        aspect_name = f\"aspect_{cluster_id}\"\n",
    "        \n",
    "        # Check against all aspect seeds\n",
    "        for aspect, seeds in ASPECT_SEEDS.items():\n",
    "            overlap = len(set(cluster_terms).intersection(seeds))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                aspect_name = aspect\n",
    "        \n",
    "        # If no strong match, try partial matches\n",
    "        if max_overlap == 0:\n",
    "            for aspect, seeds in ASPECT_SEEDS.items():\n",
    "                if any(seed in term for term in cluster_terms for seed in seeds):\n",
    "                    aspect_name = aspect\n",
    "                    break\n",
    "        \n",
    "        aspect_names[cluster_id] = aspect_name\n",
    "    \n",
    "    def map_to_cluster_aspect(terms):\n",
    "        if not terms:\n",
    "            return ['general']\n",
    "        final_aspects = []\n",
    "        for item in terms:\n",
    "            if isinstance(item, tuple) and len(item) == 2 and item[0] != 'unknown':\n",
    "                term = item[0]\n",
    "                try:\n",
    "                    prediction = kmeans.predict(vectorizer.transform([term]))[0]\n",
    "                    # Ensure prediction is within our aspect_names\n",
    "                    if prediction < n_clusters:  # Check cluster ID is valid\n",
    "                        final_aspects.append(aspect_names[prediction])\n",
    "                    else:\n",
    "                        # Find the most similar aspect\n",
    "                        term_vec = vectorizer.transform([term])\n",
    "                        closest_cluster = kmeans.predict(term_vec)[0]\n",
    "                        if closest_cluster < n_clusters:\n",
    "                            final_aspects.append(aspect_names[closest_cluster])\n",
    "                        else:\n",
    "                            final_aspects.append('general')\n",
    "                except:\n",
    "                    final_aspects.append('general')\n",
    "            elif item == 'general':\n",
    "                final_aspects.append('general')\n",
    "        return list(set(final_aspects)) if final_aspects else ['general']\n",
    "    \n",
    "    data['final_aspects'] = data['extracted_aspects'].apply(map_to_cluster_aspect)\n",
    "    return data['final_aspects'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff330d9e-a155-4802-b288-220966396f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "model_paths = [\"ensemble_absa_models_V1.pkl\"]\n",
    "models = []\n",
    "for path in model_paths:\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded = pickle.load(f)\n",
    "        if isinstance(loaded, list):\n",
    "            models.extend(loaded)  # flatten if it's a list of models\n",
    "        else:\n",
    "            models.append(loaded)\n",
    "        \n",
    "# Ensemble Voting Function\n",
    "def ensemble_predict(models, input_ids, attention_mask=None, T=2.0):\n",
    "    num_classes = models[0].fc.out_features\n",
    "    votes = np.zeros((input_ids.shape[0], num_classes))\n",
    "    with torch.no_grad():\n",
    "        layer_norm = torch.nn.LayerNorm(num_classes).to(DEVICE)\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            model_name = model.__class__.__name__\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = layer_norm(logits)\n",
    "            logits /= T\n",
    "            votes += logits.cpu().numpy()\n",
    "    final_probs = torch.softmax(torch.tensor(votes), dim=1).numpy()\n",
    "    return np.argmax(final_probs, axis=1)\n",
    "    \n",
    "# Predict a single text using the ensemble\n",
    "def predict_text_absa(text, aspects, tokenizer, models, max_length=64):\n",
    "    results = {}\n",
    "    for aspect in aspects:\n",
    "        input_text = f\"{aspect} {text}\"\n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(DEVICE)\n",
    "        attention_mask = encoding['attention_mask'].to(DEVICE)\n",
    "        pred = ensemble_predict(models, input_ids, attention_mask, T=TEMPERATURE)[0]\n",
    "        results = {\"Aspect\": aspect, \"Sentiment\": {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}[pred]}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29cb7e81-7fbf-4244-8a0d-9659d706a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to control the Flask server thread\n",
    "keep_running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2664e2c6-1653-42d6-a4f2-41f8256cd541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flask application running. Navigate to http://127.0.0.1:5000/ in your web browser.\n"
     ]
    }
   ],
   "source": [
    "# Route for the home page\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "# Route for prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if request.method == 'POST':\n",
    "        text = request.form['text']\n",
    "        # Extract aspects\n",
    "        # Discover new aspects\n",
    "        new_terms = discover_new_aspects(text)\n",
    "        print(\"Discovered new terms:\", new_terms)\n",
    "        \n",
    "         # Incorporate new terms into ASPECT_SEEDS\n",
    "        for term, count in new_terms:\n",
    "            if count > 50:  # Only add frequently occurring terms\n",
    "                if term not in set().union(*ASPECT_SEEDS.values()):\n",
    "                    ASPECT_SEEDS.setdefault(\"new_\" + term, []).append(term)\n",
    "                    \n",
    "        aspects = cluster_aspects_for_text(text, vectorizer, kmeans)\n",
    "        predicted_sentiment = predict_text_absa(text, aspects, TOKENIZER, models)\n",
    "        return render_template('result.html', text=text, predicted_sentiment=predicted_sentiment)\n",
    "\n",
    "# Function to run Flask app in a separate thread\n",
    "def run_flask_app():\n",
    "    global keep_running\n",
    "    server = make_server('127.0.0.1', 5000, app)\n",
    "    while keep_running:\n",
    "        server.handle_request()\n",
    "\n",
    "# Check if the script is run as main module\n",
    "if __name__ == '__main__':\n",
    "    # Create and start a new thread for Flask app\n",
    "    flask_thread = threading.Thread(target=run_flask_app)\n",
    "    flask_thread.start()\n",
    "    \n",
    "    # Provide a message indicating Flask is running\n",
    "    print(\"Flask application running. Navigate to http://127.0.0.1:5000/ in your web browser.\")\n",
    "    \n",
    "    # Set the flag to stop the Flask server thread\n",
    "    #keep_running = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c7b69-18ec-4d69-883b-ea31f8dedb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
